[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi there, I’m Cory! I’m a Lead Data Scientist specializing in healthcare analytics, economics, and machine learning. I’m particularly interested in industrial organization, probability, and reinforcement learning. I’ve spent my entire career working in the healthcare finance space, so that’s my primary area of expertise. This is mostly for exploring the other stuff :)"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links & Resources",
    "section": "",
    "text": "Maybe I’ll fill this out someday. In the interim, here’s Calvin and Hobbes\n\n\n\nrelativity"
  },
  {
    "objectID": "links.html#publications-projects",
    "href": "links.html#publications-projects",
    "title": "Links & Resources",
    "section": "",
    "text": "GitHub Repository\nLinkedIn Profile\nTwitter Feed"
  },
  {
    "objectID": "links.html#recommended-reading",
    "href": "links.html#recommended-reading",
    "title": "Links & Resources",
    "section": "",
    "text": "Healthcare Analytics on Medium\nNEJM Catalyst\nHealth Affairs Blog\n\n\n\n\n\nPapers With Code\nTowards Data Science\nAnalytics Vidhya\n\n\n\n\n\nMLflow Documentation\nPytorch\nscikit-learn"
  },
  {
    "objectID": "links.html#professional-organizations",
    "href": "links.html#professional-organizations",
    "title": "Links & Resources",
    "section": "",
    "text": "AMIA (American Medical Informatics Association)\nHIMSS (Healthcare Information and Management Systems Society)\nINFORMS (Institute for Operations Research and Management Sciences)\n\nNote: These are resources I find valuable in my work and research. Feel free to suggest additional resources!"
  },
  {
    "objectID": "photos.html",
    "href": "photos.html",
    "title": "Photo Gallery",
    "section": "",
    "text": "I’ll add some photos from personal and professional stuff here eventually. Probably some data viz too. These bears are very good placeholders though.\n\n\n\n\n\nBaby Bears\n\n\n\n\n\n\n\nWow Bear\n\n\n\n\n\n\n\n\n\nMore Baby Bears\n\n\n\n\n\n\n\nEven More Baby Bears"
  },
  {
    "objectID": "photos.html#conference-presentations-speaking-events",
    "href": "photos.html#conference-presentations-speaking-events",
    "title": "Photo Gallery",
    "section": "",
    "text": "Data Science Conference 2023\n\n\n\n\n\n\n\nHealthcare Analytics Summit\n\n\n\n\n\n\n\nMLOps Workshop"
  },
  {
    "objectID": "photos.html#project-visualizations",
    "href": "photos.html#project-visualizations",
    "title": "Photo Gallery",
    "section": "",
    "text": "Supply Chain Optimization Model\n\n\nNetwork visualization of healthcare supply chain optimization\n\n\n\n\n\nPatient Flow Analysis\n\n\nHealthcare system patient flow analysis dashboard"
  },
  {
    "objectID": "photos.html#recent-events",
    "href": "photos.html#recent-events",
    "title": "Photo Gallery",
    "section": "",
    "text": "Team Workshop\n\n\nData Science Team Workshop\n\n\n\n\n\nConference Panel\n\n\nHealthcare Analytics Panel Discussion\n\n\n\n\n\nResearch Presentation\n\n\nPresenting Research Findings\n\n\nNote: Photos are regularly updated to showcase recent work and events. Check back for new additions!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Welcome to my blog :)",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPapers: Dualformer\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPapers: Cost of Regulatory Compliance in the United States\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPapers: Switchback Price Experiments with Forward-Looking Demand\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Pricing in a Very Dumb Vacuum\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html",
    "href": "blog/posts/dynamic-pricing.html",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "",
    "text": "Recent advancements in artificial intelligence, particularly reinforcement learning (RL), have been touted as potentially transformative for pricing strategies in oligopolistic markets. I don’t really buy it, but wasn’t super familiar with what the process here looked like in practice. In order to Get Familiar, I put together a simple model where firms compete on price, but with different pricing strategies. We’ll get to that later: first, some background."
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html#introduction",
    "href": "blog/posts/dynamic-pricing.html#introduction",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "",
    "text": "Recent advancements in artificial intelligence, particularly reinforcement learning (RL), have been touted as potentially transformative for pricing strategies in oligopolistic markets. I don’t really buy it, but wasn’t super familiar with what the process here looked like in practice. In order to Get Familiar, I put together a simple model where firms compete on price, but with different pricing strategies. We’ll get to that later: first, some background."
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html#economic-foundations-of-pricing-strategy",
    "href": "blog/posts/dynamic-pricing.html#economic-foundations-of-pricing-strategy",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "Economic Foundations of Pricing Strategy",
    "text": "Economic Foundations of Pricing Strategy\n\nTheoretical Underpinnings\nThe economic theory of pricing in oligopolistic markets has a rich history, dating back to the seminal works of (cournot1838?) and (bertrand1883?). These models laid the groundwork for understanding strategic interactions in imperfectly competitive markets. Subsequent developments, including the theory of contestable markets (Baumol, Panzar, and Willig 1982), have further refined our understanding of pricing dynamics and market entry.\nMost firms probably don’t set prices based on first order conditions. Acquiring all relevant data is costly, and it’s not totally possible to have perfect information about consumer responses to price changes in most environments (I can’t think of any where it would be possible). That said, optimal firm behavior in an ideal environment provides a nice benchmark for evaluating alternative price-setting approaches.\n\n\nAntitrust Considerations\nAntitrust policy has long been informed by economic theory. The structure-conduct-performance paradigm, proposed by Mason (1939) and developed by Bain (1968), has been influential in shaping antitrust thinking. Similarly, the Chicago School critique, articulated by scholars like Posner (1979), emphasizes the importance of efficiency considerations in antitrust analysis.\nIn practice, antitrust authorities have developed various tools to assess potentially anticompetitive pricing behaviors, including:\n\nPredatory Pricing Tests: The Areeda-Turner test (Areeda and Turner 1975) provides a cost-based framework for identifying predatory pricing.\nMarket Concentration Measures: Indices like the Herfindahl-Hirschman Index (HHI) are used to assess market concentration (U.S. Department of Justice and Federal Trade Commission 2010).\nUpward Pricing Pressure: In merger analysis, measures of upward pricing pressure help quantify incentives for post-merger price increases (Farrell and Shapiro 2010).\n\nThese tools are imperfect, much like the models they’re based on. HHI, for example, is particularly sensitive to industry definitions. Spatial competition and diversity of product offerings can skew the results HHI yields in a way that isn’t immediately obvious."
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html#spectrum-of-pricing-approaches",
    "href": "blog/posts/dynamic-pricing.html#spectrum-of-pricing-approaches",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "Spectrum of Pricing Approaches",
    "text": "Spectrum of Pricing Approaches\n\nFirst-Order Condition (FOC) Based Approaches\nThe traditional economic approach assumes firms set prices to maximize profits, usually defined something like:\n\\[\\pi_i(Q_{D,i}(p_i),p_i) = (p_i - c_i) \\cdot Q_d(p_i)\\]\nIf we assume firm \\(i\\)’s quantity demanded (\\(Q_{D_i}(p_i)\\)) is linear in price \\(p_i\\), and marginal costs \\(c_i\\) are fixed, we’ll have a profit function that looks something like:\n\\[\\pi_i(p_i) = (p_i - c_i) \\cdot (\\alpha - \\beta \\cdot p_i) \\]\nyielding familiar FOCs:\n\\[\\frac{\\partial \\pi_i}{\\partial p_i} = (\\alpha + \\beta c_i - 2 \\beta p_i)\\]\nand optimality condition\n\\[ p_i = \\frac{\\alpha + \\beta c_i}{2\\beta} \\]\nWe’re going to add a minor complicating factor here and assume that firms compete on prices, taking other firms’ behavior as given, as well as time-to-delivery, which is exogenously determined. In the long run I’d like to tweak this model so that time-to-delivery can be minimized via bilateral contracting with a third party company (this can be thought of as selling goods on Amazon), but for now we’re keeping it simple. This whole exercise was inspired by Amazon dropshippers, for what it’s worth.\nI’m mostly curious about how these firms will behave when competitors behave suboptimally. To that end, we’ll introduce two other firm types: one that follows a heuristic, and another that sets prices dynamically using reinforcement learning. All code for this exercise can be found here:.\n\n\nHeuristic Approaches\nIn practice, many firms rely on simpler heuristic methods for price setting. These “rules of thumb” are often easier to implement and communicate within an organization. Common heuristics include:\n\nCost-Plus Pricing: \\(p_i = c_i * (1 + \\varphi_i)\\), where \\(\\varphi_i\\) is a markup. In practice the markup is often based set on a combination of margin targets and projected demand.\nCompetitor-Based Pricing: \\(p_i = \\mathbb{E}\\[p_{-i}\\] * (1 + \\gamma_i)\\) - basically the firm takes the expected market price and adjusts it up or down by some percentage \\(\\gamma\\), again often based on some combination of a margin target and projected demand.\n\nHeuristic approaches can be surprisingly effective in complex, dynamic environments (Gigerenzer and Gaissmaier 2011). They often require less detailed information about demand or competitive conditions compared to FOC-based methods. While they may not lead to theoretically optimal profits, they can be “good enough” given the costs and complexities of implementing more sophisticated approaches (Simon 1956).\n\n\nReinforcement Learning Approaches\nReinforcement learning (RL) has been proposed as a new approach to pricing strategy. In an RL framework, pricing decisions are made by an agent that learns from the outcomes of its actions over time. The key components of an RL system for pricing typically include:\n\nState space: Market conditions, competitor prices, inventory levels, etc.\nAction space: Possible prices the firm can set\nReward function: Typically the profit earned from each pricing decision\n\nProponents argue that RL approaches have the potential to capture complex, non-linear relationships in the market without requiring explicit model specification, and can adapt to changing market conditions through continuous learning. They’re probably wrong at this point in time, but RL-based approaches are interesting nonetheless.\nWhy are they ‘probably wrong’?\n\nAdaptability: Firms using FOC or heuristic approaches can and do update their strategies based on market outcomes.\nComplexity: While RL can handle complex state spaces, it’s not clear that this additional complexity leads to better outcomes in many real-world pricing scenarios.\nLearning: Traditional approaches also “learn” from data, albeit in a more structured manner.\n\nMoreover, RL strategies might essentially learn to emulate existing heuristic approaches:\n\nCompetitor-Based Pricing: An RL agent might learn to price relative to competitors - and those competitors may in turn be RL agents!\nValue-Based Pricing: Through interactions with the market, an RL agent could learn to estimate customers’ willingness to pay and price accordingly.\n\nIn this light, RL might be viewed not as a revolutionary new approach, but as a potentially more “efficient” way of implementing and combining existing pricing strategies. “Efficient” is in scare quotes because deviations from optimal behavior generate deadweight loss by definition, as well as the fact that deploying AI models at scale in the year 2024 generates non-negligible externalities."
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html#comparative-analysis-and-simulation-setup",
    "href": "blog/posts/dynamic-pricing.html#comparative-analysis-and-simulation-setup",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "Comparative Analysis and Simulation Setup",
    "text": "Comparative Analysis and Simulation Setup\nLet’s outline the model we’ll use in the simulation here.\n\nMarket Model: The general version of the model we’re looking at here has N firms employing one of three strategies:\n\nFOC-based: Given known demand and a simple production function, firms maximize profits based on first order conditions\nHeuristic: Firms use competitor-based pricing with periodic adjustments\nRL-based: Firms use a Deep Q-Network to learn pricing strategies\n\n\nand facing demand:\n\\[D_i = \\alpha - \\beta p_i + \\gamma \\sum_{j\\neq i} p_j - \\rho d_i + \\xi \\sum_{j\\neq i} d_j\\]\nwhere \\(p_i\\) is firm i’s price, \\(d_i\\) is its delivery time (exogenously determined), and \\(\\alpha, \\beta, \\xi, \\rho\\) are parameters.\n\nSimulation Dynamics: The simulation will run for T periods, with firms making pricing decisions in each period based on their respective strategies. We’re going to keep it simple and have one firm of each type, but two FOC firms (really just as a sanity check to make sure everyone’s behavior is as expected - the two FOC firms should behave very similarly)\nOutcome Measures: We will track prices, profits, and market concentration over time.\n\nAll of the code for the simulation below can be found in this repo.\n\nfrom dynamic_pricing_sim import Market, FOCFirm, HeuristicFirm, RLFirm, run_simulation, plot_results\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set up the market\ndemand_params = {\n    'alpha': 100,\n    'beta': 20,\n    'gamma': 6,\n    'xi': 0,\n    'rho': 0\n}\nmarket = Market(4,demand_params)\n\nfoc_firm = FOCFirm.FOCFirm(cost=5, market=market)\nheuristic_firm = HeuristicFirm.HeuristicFirm(cost=5, markup=0.1)\nfoc_firm_2 = FOCFirm.FOCFirm(cost=5, market=market)\nrl_hyperparams = RLFirm.RLHyperParams\nrl_hyperparams.epsilon = 0.5\nrl_firm = RLFirm.RLFirm(state_size=9, action_size=40, cost=5)\n\n\n# Set up the firms\nfirms = [\n    foc_firm,\n    foc_firm_2,\n    heuristic_firm,\n    rl_firm\n]\n\n\n\n# Set delivery times (assumed constant for simplicity)\ndelivery_times = np.array([1, 1, 1, 1])\n\n# Run the simulation\nT = 1000  \nprices_history, profits_history, demand_history = run_simulation(T, market, firms, delivery_times)\n\n# Plot the results\nfig = plot_results(prices_history, profits_history, demand_history,['FOC', 'FOC2', 'Heuristic', 'RL'])\nplt.show()\n\nC:\\Users\\Cory\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dynamic_pricing_sim\\equilibrium.py:15: RuntimeWarning: The iteration is not making good progress, as measured by the \n  improvement from the last ten iterations.\n  equilibrium_prices = fsolve(equations, initial_guess)\nC:\\Users\\Cory\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dynamic_pricing_sim\\equilibrium.py:15: RuntimeWarning: The iteration is not making good progress, as measured by the \n  improvement from the last five Jacobian evaluations.\n  equilibrium_prices = fsolve(equations, initial_guess)\n\n\nRecent avg profit: -26.15, Last price: 9.88, Last profit: -66.57\nRecent avg profit: -16.23, Last price: 9.88, Last profit: 0.00\nRecent avg profit: 26.47, Last price: 9.88, Last profit: 0.00\nRecent avg profit: 42.90, Last price: 9.88, Last profit: -8.98\nRecent avg profit: 61.30, Last price: 9.88, Last profit: 0.00\nRecent avg profit: 84.84, Last price: 9.88, Last profit: 116.33\nRecent avg profit: 90.84, Last price: 9.88, Last profit: 116.33\nRecent avg profit: 107.85, Last price: 9.88, Last profit: 116.33\nRecent avg profit: 114.01, Last price: 9.88, Last profit: 116.33\nRecent avg profit: 109.79, Last price: 9.88, Last profit: 116.33\n\n\n\n\n\n\n\n\n\nOk so the RL firm settles into a decent strategy over time that behaves reasonably well in the long-run. The FOC firms are mostly stable, as expected, and the heuristic firm is all over the place. That’s what you get for raising prices erratically for no reason! Bad profits!\nHere are some of the summary stats we get from the above:\n\navg_prices = np.mean(prices_history, axis=0)\navg_profits = np.mean(profits_history, axis=0)\nprice_volatility = np.std(prices_history, axis=0)\n\nprint(\"Average Prices:\")\nfor i, firm_type in enumerate(['FOC', 'FOC2', 'Heuristic', 'RL']):\n    print(f\"{firm_type} Firm: {avg_prices[i]:.2f}\")\n\nprint(\"\\nAverage Profits:\")\nfor i, firm_type in enumerate(['FOC', 'FOC2', 'Heuristic', 'RL']):\n    print(f\"{firm_type} Firm: {avg_profits[i]:.2f}\")\n\nAverage Prices:\nFOC Firm: 9.57\nFOC2 Firm: 9.55\nHeuristic Firm: 17.68\nRL Firm: 7.29\n\nAverage Profits:\nFOC Firm: 74.97\nFOC2 Firm: 74.77\nHeuristic Firm: -0.01\nRL Firm: 59.56"
  },
  {
    "objectID": "blog/posts/dynamic-pricing.html#critical-evaluation-and-antitrust-implications",
    "href": "blog/posts/dynamic-pricing.html#critical-evaluation-and-antitrust-implications",
    "title": "Dynamic Pricing in a Very Dumb Vacuum",
    "section": "Critical Evaluation and Antitrust Implications",
    "text": "Critical Evaluation and Antitrust Implications\nThis simulation setup allows us to critically evaluate several key questions:\n\nHow do RL-based pricing strategies perform compared to traditional FOC-based and heuristic approaches in terms of firm profitability and market outcomes?\n\n\nObviously the firm that is literally optimizing/maximizing profit wins - the rules of the game here are skewed. That said, the RL firm significantly outperforms our (admittedly dumb) heuristic in this simple model\n\n\nDo RL strategies converge to known optimal strategies in simple market settings? If so, do they offer any advantage over traditional methods?\n\n\nIt’s obvious that this simple RL agent does not learn the optimal pricing strategy. It’s also a bit of a pain in the neck to calibrate - it took me a decent amount of time to get to a point where the model would get consistently reasonable results - I ended up having to drop the delivery times from demand, at least for this run, to get the RL agent to deliver non-negative profits. A decent amount of reward shaping was also required.\n\n\nIn more complex settings, do RL strategies discover genuinely novel pricing strategies, or do they essentially learn to implement sophisticated versions of known heuristics?\n\n\nThis project took me a few hours of work to get up and running, and frankly at the end of it I’m not totally sure what strategy the agent settled on. My hunch is that it gets stuck in a local optimum - there were scenarios where it appeared to be maximizing quantity demanded as opposed to profit, but the relative simplicity of the model and limited training time makes me think that it was just converging to a local optimum.\n\n\nHow does the presence of RL-based pricing affect market dynamics and the behavior of firms using other pricing strategies?\n\n\nThe FOC firms aren’t really impacted by the RL firm - given non-zero price response by design, the FOC firm does exhibit some (very small) price volatility, but ultimately the consistency is well on display. Heuristic firms, however, become fairly erratic. I think this is probably a reasonable approximation of reality - firms with a well-established pricing strategy based on knowledge of their consumers and internal operation are going to know how to respond to competitors’ erratic behavior, whereas firms who are pricing based solely on perceptions of market conditions are going to struggle a bit more. In reality, the RL firm would likely train on historical firm data as well as publicly available price histories, so the ‘exploration’ periods would likely happen ‘in the lab’. That said, in practice dynamic pricing models are much more intricate than the one presented here, and in turn much more opaque - and less predictable in the face of shocks.\n\n\nWhat are the computational and data requirements for implementing RL-based pricing, and how do these compare to the requirements for sophisticated implementations of traditional approaches?\n\n\nSo if you’re putting together a silly lil model on your personal laptop, this simulation runs start-to-finish in about 5 seconds. And compared to some of the other work I’ve done in the pricing space, it was a pain in the neck to set up and calibrate. As nice as it would be to get something like this set up right and let it try to solve my own \\(\\pi-max\\) problems, I’m reminded of the adage that’s been circulating a lot with the rise of the LLM: \n\nUltimately theese things need a ton of guardrails to avoid erratic behavior, and these guardrails ultimately serve to simulate the optimal behavior we’re trying to avoid modeling in the first place. There are a bunch of other non-theoretical reasons, like customer retention or avoidance of tacit collusive behavior, that we might not want to outsource this work too. So like, maybe we should just do the work. I can think of a dozen real-world use cases of RL in an operations management setting, but price setting doesn’t seem like a great candidate.\nOn the topic of tacit collusion, from an antitrust perspective it’s super important to understand whether RL algorithms fundamentally change the nature of market competition. Can RL algorithms learn to coordinate on high markups without explicit collusion? If so, is this fundamentally different from tacit collusion that can arise with traditional pricing methods? The former seems likely, especially if you have two agents functionally following the same strategy by being trained with the same data. There’s an interesting segment on quant work from Good News that talks a bit about how the algorithms quants use at finance firms\n\nin particular\n\nSo if you’re … yapping about this wacky new trading strategy you found, they’re going to go implement it at their firm and you’re gonna lose all your edge\n\nThere are fundamentally two types of model variation here: features incorporated and model architecture. My gut says that in most of the ways that matter, these two should converge in the long run - if one model outperforms another, competitors will work to identify where the advantage is coming from and adopt it. It’s somewhere between efficient markets and no-free lunch: in the long-run, the models are the same. Once we’ve extracted all the juice from dynamic pricing we’re left where we started, good old cost minimization.\nFirms considering implementing these strategies are likely not cost-constrained, but the cost of training and serving these models is non-negligible and should probably incorporated into the broader ROI calc when it comes to incorporating these as an alternative to more traditional price-setting strategies.\nMost of what I’ve learned in this exercise was basically how to go about setting up a dynamic pricing agent, as most of my background in RL previously was thinking about how roombas should work or trying to figure out why a Q-learning agent I birthed sucks so bad at jumping over the green pipe that’s just a little too tall.\nWas it otherwise valuable? My take is that there are basically three conditions you need to meet for this to be worth thinking about:\n1. Your firm is already on a cloud services provider that makes deployment at scale easy\n2. That cloud environment also has well-constructed feature store that'd make these model builds easier\n3. You have a ton of time on your hands and have already worked through the stuff that you *know* is going to add value\nIf you can check all three boxes here, this is probably a great exercise - especially if you’re able to incorporate sentiment analysis into the pipeline. There are very few industries where attaining FOC outcomes are realistic, and it’s totally plausible that well-designed RL-based pricing could mark an improvement over existing workflows, particularly if your pricing/strategy teams are using bad heuristics (like taking a fixed markup percentage over competitor prices). These things are a bit opaque though, and in practice I will likely stick toward more interpretable models that incorporate what all parties can agree are the relevant structural features."
  },
  {
    "objectID": "blog/posts/wir-20241025.html",
    "href": "blog/posts/wir-20241025.html",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "I just finished Christopher Buehlman’s Between Two Fires. This was a really fun one - a bit longer than I usually go for but it was highly recommended. The two most dominant personalities in the book reminded me a lot of my favorite beats in Game of Thrones - namely the dynamic between the Hound and Arya Stark. Atmospherically wonderful, and the way the monsters are written reminded me of two of my favorite projects of the past few years, Aesop Rock’s Spirit World Field Guide and John Langan’s The Fisherman. If you’re into horror, Between Two Fires might be my top recommend right now.\nLoved this book.\n\n\n\n\n\n\n\nI picked this up because Ben Recht’s Patterns Predictions and Actions was availabe at a steep discount on the Princeton University Press website (sidenote: pick this one up - it’s excellent). I started reading it while waiting for my next horror story to come in the mail - but it’s on pause for the rest of October\n\n\n\n\nI spent a few hours scouring Amazon for something to read - this seemed fun and in the vein of what I was after. It’s also a little longer than I’m usually into - we’ll see how I’m feeling at the end of the month. My initial impression is that this one might not be for me, but I try to get at least halfway through everything.\n\n\nYou can find me on StoryGraph if you want to connect over any of these.\n\n\n\n\n\n\nI’ve been going down a bit of a dynamic pricing rabbit hole - something about spending time clothes shopping on ebay and book shopping on Amazon has me thinking about how hard it is to get pricing right. My last blog post covers some early thoughts with simulations, and I’ve spent some time poking around Arxiv to see where dynamic pricing is right now. There’s some really nice theoretical work out this week from a research group at Stanford Wu, Johari, Syrgkanis, and Weintraub’s Switchback Price Experiments with Forward-Looking Demand - they prove that standard two-price experiments cannot identify the true demand gradient in a simple setting with forward-looking consumers while a three-price design can provide unbiased estimates. The simplicity of the environment makes me skeptical that a structural implementation of this model would truly be ‘unbiased’ – the estimation here is likely too generous to consumers who are often more myopic than modeled … so I’d imagine that the contours estimated under this approach yield higher elasticities than we’d see in practice. I’ve got some simulation work thinking about this too - my practical OR experience is more in the cost-min and efficiency maximizing spaces, and in grad school the focus was more on equilibrium outcomes than these sort of pricing strategies. I’d like to see what this looks like with (1) myopic consumers, (2) an entry threat, and (3) inventory constraints.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nClean identification result that provides useful bounds on demand elasticity\nNovel solution to a fundamental measurement problem, even if assumptions about consumer behavior are strong\nThree-price design is simple enough to actually implement, unlike most theoretical solutions to strategic consumer problems\nSmart positioning at intersection of economics and operations research\nCould be extended to incorporate competing firms and inventory constraints, though that might break some of the nice theoretical properties\n\n\n\n\n\n\nI also spent some time thinking about compliance - in particular Bombardini, Trebbi, and Zhang’s Measuring the Costs and Benefits of Regulation, and Trebbi and Zhang’s The Cost of Regulatory Compliance in the United States. Broadly, the former outlines where we’re at in terms of evaluating the impact of regulation while the latter implements an interesting method that blends O*Net data with employment data outlining on-the-job activities, identifying tasks that are regulation-related (like “evaluate applications for compliance” or “monitor adherence to safety standards”), figuring out what percentage of each occupation’s time is spent on regulatory tasks, and then calculating what percentage of a company’s total wage bill goes to regulatory compliance. They argue that the cost of regulation exhibits a U-shaped relationship with regulatory costs - medium-sized companies spend the highest percentage on compliance.\n\n[They] observe that over time, the inverted-U shape relation between RegIndex and firm size became stronger. In particular, much of the changes in RegIndex (the authors’ derived measure of the percentage of an establishment’s total labor spending ascribed to performing regulation-related tasks) come from firms with a medium and high level of employment, while there is little change in RegIndex for small firms. Importantly, this enhanced inversed-U relationship between RegIndex and size maps to a greater average log change in RegIndex for larger firms\n\nThis suggests that neither really small companies (who might fly under the radar) nor really big ones (who can achieve economies of scale in compliance) bear the heaviest regulatory burden. From an identification perspective, the authors employ two main identification strategies:\n\nA series of event studies/diff-in-diff around major regulatory changes, like:\n\nCredit CARD Act of 2009 affecting credit card issuers\nEnergy Policy Act of 2005 and subsequent re-regulation of oil & gas\nDear Colleague Letter of 2011 affecting colleges\nACA affecting hospitals\n\nA shift-share instrument approach to separate enforcement from regulatory requirements. They:\n\nMeasure regulatory requirement changes via new regulations from each agency\nMeasure enforcement changes via changes in regulatory-related employment at agencies\nCreate Bartik-style instruments based on industry exposure to different agencies\n\n\nThe event studies seem decent for validation - they show RegIndex moves in expected directions when regulations change. Their measure also captures both increases AND decreases in regulation, but I’m not super convinced by their causal claims about effects on firm size distribution. The inverted-U shape in regulatory costs by firm size is descriptive, and while they try to use their instruments to understand whether it’s driven by enforcement vs requirements, there are still lots of potential confounders. For instance, mid-sized firms might have different business models or operate in different sectors that require more compliance regardless of firm size per se.\nI didn’t have high hopes for this paper when the first hit on Google was a writeup by the authors hosted by Cato, but it was at the very least interesting.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nKey contribution is new measurement approach: using actual labor allocation data to measure regulatory compliance costs\nFind mid-sized firms (~500 employees) bear highest relative regulatory burden\nIdentification comes from regulatory event studies and shift-share design\nMain limitations are potential overestimation of compliance time and missing non-labor costs\nValidates well against known regulatory changes but causal claims about firm size effects remain uncertain\n\n\n\n\n\n\nThe title rage-baited me. Meta’s new Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces develops a training approach motivated by Kahneman’s two systems theory - which is, to me, basically the thinking man’s evolutionary psychology: widely held as false by academics active in the field, but digestible enough that everyone loves citing it. Paul Glimcher has some fantastic lectures that break down why the evidence just doesn’t support the two-systems model:\n\n  \n\nThe actual technical meat is interesting despite the motivation: given training data of successfully solved tasks (think mazes or math problems) that includes all solution steps, selectively chop off pieces that aren’t crucial for finding the answer. This pruned version simulates the “fast brain” while the complete solution process represents the “slow brain.”\nIt’s fine. But honestly, if you have that kind of training data, why not just train an RL agent? The requirements are basically the same - you need tons of examples of successful problem-solving - but RL gives you a more principled framework for learning optimal behavior.\n\n\n\n\n\nflowchart LR\n    subgraph RLSystem[\"RL System\"]\n        agent[\"Agent\"]\n        exp[\"Experience Buffer\"]\n    end\n\n    subgraph DualSystem[\"Dualformer\"]\n        fast[\"Fast Path\"]\n        slow[\"Slow Path\"]\n        trace[\"Trace Processing\"]\n    end\n\n    subgraph Environment[\"Environment\"]\n        state[\"State Space\"]\n        reward[\"Reward Signal\"]\n    end\n\n    agent --&gt; state\n    state --&gt; reward\n    reward --&gt; agent\n    agent --&gt; exp\n    exp --&gt; trace\n    trace --&gt; fast\n    trace --&gt; slow\n    fast --&gt; agent\n    slow --&gt; agent\n\n    classDef rl fill:#bbf,stroke:#333,stroke-width:2px\n    classDef dual fill:#fbb,stroke:#333,stroke-width:2px\n    classDef env fill:#bfb,stroke:#333,stroke-width:2px\n    \n    class RLSystem rl\n    class DualSystem dual\n    class Environment env\n\n\n\n\n\n\nThere might be something interesting in combining an RL agent with their architecture though. Imagine co-training the agent and the Dualformer so it learns both quick-and-dirty and careful-and-thorough approaches while the agent bumbles its way to solutions. The diagram above shows how this might work - the RL agent feeds experiences into the Dualformer, which learns both fast and slow solving strategies that can then guide future exploration.\nThe paper isn’t bad, and I’m sure it’s interesting and valuable to people smarter than me/folks who actually work on transformers day-to-day - the technical work, at a minimum, is solid. But they successfully annoyed me into reading something that over-promised and under-delivered by wrapping a fairly straightforward transformer training technique in cognitive science gift paper.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nSolid technical contribution wrapped in questionable cognitive science\nProbably should’ve just used RL\nPotential for interesting hybrid architectures (though that wasn’t their point)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy partner and I just finished Better Call Saul - a show that’s impossible for me to totally disentangle from Breaking Bad - but it has a distinct advantage for me over arguably the greatest show of all time: I actually liked the main character. In fact, I liked most of the characters in this show. I caught the first 2.5 seasons live, but just now circled back to actually watch/finish it, and I’m not sure what to expect from these guys other than perfection at this point.\n\n\n\n\n\n\n\n\n\n\n\nI’ve been on Wellington Paranormal at bedtime, a marked jump from my annual October What We Do In The Shadows rewatch. WP is short and breezy, but very fun. Would recommend.\nI think season 1 of The Sinner is up next. NBA will be bedtime TV for the foreseeable future. Pray for the knees of Paul George and Joel Embiid.\n\n\n\n\n\n\n\nMy favorite of the week. Watched after searching for ‘creature features’ and honestly a little offensive to characterize in that way. It’s got Justin Long!\n\n\n\n\nThe Canadian Terror\n\n\n\n\nAbsolute classic banger\n\n\nBe my friend on Letterboxd :)\n\n\n\n\nNew Bartees Strange, Samantha Fish cover of I Put a Spell on You. Lots of fun to be had around Halloween. Autumn always bring Jason Isbell back to my playlists, but this might be the first October where Cast Iron Skillet supplants Speed Trap Town in my yearning rankings. Unprecedented times.\n\n\n\n \n\nDon't wash the cast iron skillet\nDon't drink and drive, you'll spill it\nDon't ask too many questions or you'll never get to sleep\nThere's a hole inside you, fill it\n\n\n\n\nWrapped a dinky model with an RL agent executing a dynamic pricing strategy. There are about a dozen things wrong with it, but it was a fun exercise. I wrote about the thought process and results here. I also spent some time playing with an old Mario RL agent I put together that combined human-in-the-loop with a DQN agent. Finally at a point where you can trade off with the agent after a 5-second lapse in human input. Unfortunately the agent is terrible at Mario and gets hit by koopas/goombas immediately. I might fiddle with these a bit more but I’m content where they’re at for now.\nNext is to implement the switchback simulation with entry threat as mentioned above. Work work is work work - focused on GCP migration, learned a bit about the mechanics of some wonky HEDIS measures.\n\n\n\nBen Recht’s Cone Programming: I mentioned his book earlier, but his blog is also great. Love how he shows the progression from simple nonnegative orthants to second-order and semidefinite cones, highlighting the self-duality properties that make optimization over these sets work. For an operations research perspective on why this theory matters, check out this excellent guide to conic optimization by Letchford and Parkes - they go a bit more in-depth on the theory and discuss a few important applications.\nAmazon is at it again — Jules Roscoe at 404Media discusses their latest legal tactic: claiming a constitutional right to hold anti-union meetings and refuse to post worker rights notices.\n\n“It’s a ridiculous argument,”” said Seth Goldstein, a lawyer at Goldstein & Singla PLLC, who represents workers in unionization efforts. Goldstein previously represented Amazon workers who unionized in a Staten Island warehouse. “What I don’t understand is, the First Amendment is about government: the government can’t restrain free speech. In this case, are they really saying that the NLRB, as a government agency, is somehow limiting free speech? Constitutional rights are usually outside of the framework of private employers. I think it turns everything on its head.””\n\nI feel like lots of folks are missing this point these days… This is part of a growing corporate assault on NLRB oversight, with SpaceX and Trader Joe’s making similar claims. For context on what they’re really fighting against, Luis Feliz Leon at LaborNotes has an excellent overview of the current wave of Amazon unionization efforts.\nI’ve spent more time than I should have on The Ringer’s Fantasy Football Rankings the past few weeks."
  },
  {
    "objectID": "blog/posts/wir-20241025.html#reading",
    "href": "blog/posts/wir-20241025.html#reading",
    "title": "Weekly Notes: 2024-10-22",
    "section": "",
    "text": "I just finished Christopher Buehlman’s Between Two Fires. This was a really fun one - a bit longer than I usually go for but it was highly recommended. The two most dominant personalities in the book reminded me a lot of my favorite beats in Game of Thrones - namely the dynamic between the Hound and Arya Stark. Atmospherically wonderful, and the way the monsters are written reminded me of two of my favorite projects of the past few years, Aesop Rock’s **Spirit World Field Guide* and John Langan’s The Fisherman. If you’re into horror, Between Two Fires might be my top recommend right now.\nLoved this book.\n\n\n\n\n\n\n\nI picked this up because Ben Recht’s Patterns Predictions and Actions was availabe at a steep discount on the Princeton University Press website (sidenote: pick this one up - it’s excellent). I started reading it while waiting for my next horror story to come in the mail - but it’s on pause for the rest of October\n\n\n\n\nI spent a few hours scouring Amazon for something to read - this seemed fun and in the vein of what I was after. It’s also a little longer than I’m usually into - we’ll see how I’m feeling at the end of the month. My initial impression is that this one might not be for me, but I try to get at least halfway through everything."
  },
  {
    "objectID": "blog/posts/wir-20241025.html#watching",
    "href": "blog/posts/wir-20241025.html#watching",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "My partner and I just finished Better Call Saul - a show that’s impossible for me to totally disentangle from Breaking Bad - but it has a distinct advantage for me over arguably the greatest show of all time: I actually liked the main character. In fact, I liked most of the characters in this show. I caught the first 2.5 seasons live, but just now circled back to actually watch/finish it, and I’m not sure what to expect from these guys other than perfection at this point.\n\n\n\n\n\n\n\n\n\n\n\nI’ve been on Wellington Paranormal at bedtime, a marked jump from my annual October What We Do In The Shadows rewatch. WP is short and breezy, but very fun. Would recommend.\nI think season 1 of The Sinner is up next. NBA will be bedtime TV for the foreseeable future. Pray for the knees of Paul George and Joel Embiid.\n\n\n\n\n\n\n\nMy favorite of the week. Watched after searching for ‘creature features’ and honestly a little offensive to characterize in that way. It’s got Justin Long!\n\n\n\n\nThe Canadian Terror\n\n\n\n\nAbsolute classic banger\n\n\nBe my friend on Letterboxd :)"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#listening",
    "href": "blog/posts/wir-20241025.html#listening",
    "title": "Week in Review: 10/25/2024",
    "section": "",
    "text": "Notable episodes I enjoyed:\n\nEpisode name, show, and key takeaways"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#joy-journal",
    "href": "blog/posts/wir-20241025.html#joy-journal",
    "title": "Week in Review: 10/25/2024",
    "section": "",
    "text": "Important\n\n\n\nMoments that made me smile this week\n\n\n\n\n\n\n\n\nImage 1 Description\n\n\n\n\n\n\n\nImage 2 Description\n\n\n\n\n\n\n\nImage 3 Description"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#random-thoughts",
    "href": "blog/posts/wir-20241025.html#random-thoughts",
    "title": "Week in Review: 10/25/2024",
    "section": "",
    "text": "Interesting ideas that crossed my mind\nQuestions I’m pondering\nFuture topics to explore"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#links-worth-sharing",
    "href": "blog/posts/wir-20241025.html#links-worth-sharing",
    "title": "Week in Review: 10/25/2024",
    "section": "",
    "text": "Interesting things I discovered online:\n\nLink 1: Brief description\nLink 2: Brief description\nLink 3: Brief description"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#books",
    "href": "blog/posts/wir-20241025.html#books",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "I just finished Christopher Buehlman’s Between Two Fires. This was a really fun one - a bit longer than I usually go for but it was highly recommended. The two most dominant personalities in the book reminded me a lot of my favorite beats in Game of Thrones - namely the dynamic between the Hound and Arya Stark. Atmospherically wonderful, and the way the monsters are written reminded me of two of my favorite projects of the past few years, Aesop Rock’s Spirit World Field Guide and John Langan’s The Fisherman. If you’re into horror, Between Two Fires might be my top recommend right now.\nLoved this book.\n\n\n\n\n\n\n\nI picked this up because Ben Recht’s Patterns Predictions and Actions was availabe at a steep discount on the Princeton University Press website (sidenote: pick this one up - it’s excellent). I started reading it while waiting for my next horror story to come in the mail - but it’s on pause for the rest of October\n\n\n\n\nI spent a few hours scouring Amazon for something to read - this seemed fun and in the vein of what I was after. It’s also a little longer than I’m usually into - we’ll see how I’m feeling at the end of the month. My initial impression is that this one might not be for me, but I try to get at least halfway through everything.\n\n\nYou can find me on StoryGraph if you want to connect over any of these."
  },
  {
    "objectID": "blog/posts/wir-20241025.html#papers",
    "href": "blog/posts/wir-20241025.html#papers",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "I’ve been going down a bit of a dynamic pricing rabbit hole - something about spending time clothes shopping on ebay and book shopping on Amazon has me thinking about how hard it is to get pricing right. My last blog post covers some early thoughts with simulations, and I’ve spent some time poking around Arxiv to see where dynamic pricing is right now. There’s some really nice theoretical work out this week from a research group at Stanford Wu, Johari, Syrgkanis, and Weintraub’s Switchback Price Experiments with Forward-Looking Demand - they prove that standard two-price experiments cannot identify the true demand gradient in a simple setting with forward-looking consumers while a three-price design can provide unbiased estimates. The simplicity of the environment makes me skeptical that a structural implementation of this model would truly be ‘unbiased’ – the estimation here is likely too generous to consumers who are often more myopic than modeled … so I’d imagine that the contours estimated under this approach yield higher elasticities than we’d see in practice. I’ve got some simulation work thinking about this too - my practical OR experience is more in the cost-min and efficiency maximizing spaces, and in grad school the focus was more on equilibrium outcomes than these sort of pricing strategies. I’d like to see what this looks like with (1) myopic consumers, (2) an entry threat, and (3) inventory constraints.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nClean identification result that provides useful bounds on demand elasticity\nNovel solution to a fundamental measurement problem, even if assumptions about consumer behavior are strong\nThree-price design is simple enough to actually implement, unlike most theoretical solutions to strategic consumer problems\nSmart positioning at intersection of economics and operations research\nCould be extended to incorporate competing firms and inventory constraints, though that might break some of the nice theoretical properties\n\n\n\n\n\n\nI also spent some time thinking about compliance - in particular Bombardini, Trebbi, and Zhang’s Measuring the Costs and Benefits of Regulation, and Trebbi and Zhang’s The Cost of Regulatory Compliance in the United States. Broadly, the former outlines where we’re at in terms of evaluating the impact of regulation while the latter implements an interesting method that blends O*Net data with employment data outlining on-the-job activities, identifying tasks that are regulation-related (like “evaluate applications for compliance” or “monitor adherence to safety standards”), figuring out what percentage of each occupation’s time is spent on regulatory tasks, and then calculating what percentage of a company’s total wage bill goes to regulatory compliance. They argue that the cost of regulation exhibits a U-shaped relationship with regulatory costs - medium-sized companies spend the highest percentage on compliance.\n\n[They] observe that over time, the inverted-U shape relation between RegIndex and firm size became stronger. In particular, much of the changes in RegIndex (the authors’ derived measure of the percentage of an establishment’s total labor spending ascribed to performing regulation-related tasks) come from firms with a medium and high level of employment, while there is little change in RegIndex for small firms. Importantly, this enhanced inversed-U relationship between RegIndex and size maps to a greater average log change in RegIndex for larger firms\n\nThis suggests that neither really small companies (who might fly under the radar) nor really big ones (who can achieve economies of scale in compliance) bear the heaviest regulatory burden. From an identification perspective, the authors employ two main identification strategies:\n\nA series of event studies/diff-in-diff around major regulatory changes, like:\n\nCredit CARD Act of 2009 affecting credit card issuers\nEnergy Policy Act of 2005 and subsequent re-regulation of oil & gas\nDear Colleague Letter of 2011 affecting colleges\nACA affecting hospitals\n\nA shift-share instrument approach to separate enforcement from regulatory requirements. They:\n\nMeasure regulatory requirement changes via new regulations from each agency\nMeasure enforcement changes via changes in regulatory-related employment at agencies\nCreate Bartik-style instruments based on industry exposure to different agencies\n\n\nThe event studies seem decent for validation - they show RegIndex moves in expected directions when regulations change. Their measure also captures both increases AND decreases in regulation, but I’m not super convinced by their causal claims about effects on firm size distribution. The inverted-U shape in regulatory costs by firm size is descriptive, and while they try to use their instruments to understand whether it’s driven by enforcement vs requirements, there are still lots of potential confounders. For instance, mid-sized firms might have different business models or operate in different sectors that require more compliance regardless of firm size per se.\nI didn’t have high hopes for this paper when the first hit on Google was a writeup by the authors hosted by Cato, but it was at the very least interesting.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nKey contribution is new measurement approach: using actual labor allocation data to measure regulatory compliance costs\nFind mid-sized firms (~500 employees) bear highest relative regulatory burden\nIdentification comes from regulatory event studies and shift-share design\nMain limitations are potential overestimation of compliance time and missing non-labor costs\nValidates well against known regulatory changes but causal claims about firm size effects remain uncertain\n\n\n\n\n\n\nThe title rage-baited me. Meta’s new Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces develops a training approach motivated by Kahneman’s two systems theory - which is, to me, basically the thinking man’s evolutionary psychology: widely held as false by academics active in the field, but digestible enough that everyone loves citing it. Paul Glimcher has some fantastic lectures that break down why the evidence just doesn’t support the two-systems model:\n\n  \n\nThe actual technical meat is interesting despite the motivation: given training data of successfully solved tasks (think mazes or math problems) that includes all solution steps, selectively chop off pieces that aren’t crucial for finding the answer. This pruned version simulates the “fast brain” while the complete solution process represents the “slow brain.”\nIt’s fine. But honestly, if you have that kind of training data, why not just train an RL agent? The requirements are basically the same - you need tons of examples of successful problem-solving - but RL gives you a more principled framework for learning optimal behavior.\n\n\n\n\n\nflowchart LR\n    subgraph RLSystem[\"RL System\"]\n        agent[\"Agent\"]\n        exp[\"Experience Buffer\"]\n    end\n\n    subgraph DualSystem[\"Dualformer\"]\n        fast[\"Fast Path\"]\n        slow[\"Slow Path\"]\n        trace[\"Trace Processing\"]\n    end\n\n    subgraph Environment[\"Environment\"]\n        state[\"State Space\"]\n        reward[\"Reward Signal\"]\n    end\n\n    agent --&gt; state\n    state --&gt; reward\n    reward --&gt; agent\n    agent --&gt; exp\n    exp --&gt; trace\n    trace --&gt; fast\n    trace --&gt; slow\n    fast --&gt; agent\n    slow --&gt; agent\n\n    classDef rl fill:#bbf,stroke:#333,stroke-width:2px\n    classDef dual fill:#fbb,stroke:#333,stroke-width:2px\n    classDef env fill:#bfb,stroke:#333,stroke-width:2px\n    \n    class RLSystem rl\n    class DualSystem dual\n    class Environment env\n\n\n\n\n\n\nThere might be something interesting in combining an RL agent with their architecture though. Imagine co-training the agent and the Dualformer so it learns both quick-and-dirty and careful-and-thorough approaches while the agent bumbles its way to solutions. The diagram above shows how this might work - the RL agent feeds experiences into the Dualformer, which learns both fast and slow solving strategies that can then guide future exploration.\nThe paper isn’t bad, and I’m sure it’s interesting and valuable to people smarter than me/folks who actually work on transformers day-to-day - the technical work, at a minimum, is solid. But they successfully annoyed me into reading something that over-promised and under-delivered by wrapping a fairly straightforward transformer training technique in cognitive science gift paper.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nSolid technical contribution wrapped in questionable cognitive science\nProbably should’ve just used RL\nPotential for interesting hybrid architectures (though that wasn’t their point)"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#tunes",
    "href": "blog/posts/wir-20241025.html#tunes",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "New Bartees Strange, Samantha Fish cover of I Put a Spell on You. Lots of fun to be had around Halloween. Autumn always bring Jason Isbell back to my playlists, but this might be the first October where Cast Iron Skillet supplants Speed Trap Town in my yearning rankings. Unprecedented times.\n\n\n\n \n\nDon't wash the cast iron skillet\nDon't drink and drive, you'll spill it\nDon't ask too many questions or you'll never get to sleep\nThere's a hole inside you, fill it"
  },
  {
    "objectID": "blog/posts/wir-20241025.html#links-from-da-net",
    "href": "blog/posts/wir-20241025.html#links-from-da-net",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "Ben Recht’s Cone Programming: I mentioned his book earlier, but his blog is also great. Love how he shows the progression from simple nonnegative orthants to second-order and semidefinite cones, highlighting the self-duality properties that make optimization over these sets work. For an operations research perspective on why this theory matters, check out this excellent guide to conic optimization by Letchford and Parkes - they go a bit more in-depth on the theory and discuss a few important applications.\nAmazon is at it again — Jules Roscoe at 404Media discusses their latest legal tactic: claiming a constitutional right to hold anti-union meetings and refuse to post worker rights notices.\n\n“It’s a ridiculous argument,”” said Seth Goldstein, a lawyer at Goldstein & Singla PLLC, who represents workers in unionization efforts. Goldstein previously represented Amazon workers who unionized in a Staten Island warehouse. “What I don’t understand is, the First Amendment is about government: the government can’t restrain free speech. In this case, are they really saying that the NLRB, as a government agency, is somehow limiting free speech? Constitutional rights are usually outside of the framework of private employers. I think it turns everything on its head.””\n\nI feel like lots of folks are missing this point these days… This is part of a growing corporate assault on NLRB oversight, with SpaceX and Trader Joe’s making similar claims. For context on what they’re really fighting against, Luis Feliz Leon at LaborNotes has an excellent overview of the current wave of Amazon unionization efforts.\nI’ve spent more time than I should have on The Ringer’s Fantasy Football Rankings the past few weeks."
  },
  {
    "objectID": "blog/posts/wir-20241025.html#workin-on",
    "href": "blog/posts/wir-20241025.html#workin-on",
    "title": "Week in Review: 10/24/2024",
    "section": "",
    "text": "Wrapped a dinky model with an RL agent executing a dynamic pricing strategy. There are about a dozen things wrong with it, but it was a fun exercise. I wrote about the thought process and results here. I also spent some time playing with an old Mario RL agent I put together that combined human-in-the-loop with a DQN agent. Finally at a point where you can trade off with the agent after a 5-second lapse in human input. Unfortunately the agent is terrible at Mario and gets hit by koopas/goombas immediately. I might fiddle with these a bit more but I’m content where they’re at for now.\nNext is to implement the switchback simulation with entry threat as mentioned above. Work work is work work - focused on GCP migration, learned a bit about the mechanics of some wonky HEDIS measures."
  },
  {
    "objectID": "blog/posts/switchback_experiments.html",
    "href": "blog/posts/switchback_experiments.html",
    "title": "Papers: Switchback Price Experiments with Forward-Looking Demand",
    "section": "",
    "text": "I’ve been going down a bit of a dynamic pricing rabbit hole - Black Friday is right around the corner and with so many retailers offering some kind of discount it’s increasingly difficult to identify value, even as a ‘well-informed’ consumer. My last blog post covers some early thoughts with simulations, and I’ve spent some time poking around Arxiv to see where dynamic pricing is right now. There’s some really nice theoretical work out this week from a research group at Stanford Wu, Johari, Syrgkanis, and Weintraub’s Switchback Price Experiments with Forward-Looking Demand - they prove that standard two-price experiments cannot identify the true demand gradient in a simple setting with forward-looking consumers while a three-price design can provide unbiased estimates. The simplicity of the environment makes me skeptical that a structural implementation of this model would truly be ‘unbiased’ – the estimation here is likely too generous to consumers who are often more myopic than modeled … so I’d imagine that the contours estimated under this approach yield higher elasticities than we’d see in practice. I’ve got some simulation work thinking about this too - my practical OR experience is more in the cost-min and efficiency maximizing spaces, and in grad school the focus was more on equilibrium outcomes than these sort of pricing strategies. I’d like to see what this looks like with (1) myopic consumers, (2) an entry threat, and (3) inventory constraints. I’ve put some work together in this direction, planning on fleshing this out a bit more and trying to think through some of these implications more in-depth. Myopic consumers seem the most directly impactful for the theory considered here – the authors postulate an infinite number of consumers with perfect foresight when in actuality there’s likely a relatively small number of people who’ve got your product in their Amazon ‘saved for later’ list and look at it periodically and any targeted pings about discounts on your product are likely drowned out by social media and other applications’ notifications that they’re unlikely to be particularly impactful. Inventory constraints are theoretically irrelevant in terms of demand gradient estimation but the implied cost adjustment likely impacts the optimal discounts offered - which should really be emphasized here. A 20% discount, even for a comparatively small subset of potential consumers could drive a small firm into the red. There’s a similar issue with entry threats, but the presence of competitors (even in a market with differentiated products) has implications from an efficient rationing perspective that likely distort the estimated gradient.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nClean identification result that provides useful bounds on demand elasticity\nNovel solution to a fundamental measurement problem, even if assumptions about consumer behavior are strong\nThree-price design is simple enough to actually implement, unlike most theoretical solutions to strategic consumer problems\nSmart positioning at intersection of economics and operations research\nCould be extended to incorporate competing firms and inventory constraints, though that might break some of the nice theoretical properties"
  },
  {
    "objectID": "blog/posts/dualformer.html",
    "href": "blog/posts/dualformer.html",
    "title": "Papers: Dualformer",
    "section": "",
    "text": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces\nThe title rage-baited me. Meta’s new Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces develops a training approach motivated by Kahneman’s two systems theory - which is, to me, basically the thinking man’s evolutionary psychology: widely held as false by academics active in the field, but digestible enough that everyone loves citing it. Paul Glimcher has some fantastic lectures that break down why the evidence just doesn’t support the two-systems model:\n\n  \n\nThe actual technical meat is interesting despite the motivation: given training data of successfully solved tasks (think mazes or math problems) that includes all solution steps, selectively chop off pieces that aren’t crucial for finding the answer. This pruned version simulates the “fast brain” while the complete solution process represents the “slow brain.”\nIt’s fine. But honestly, if you have that kind of training data, why not just train an RL agent? The requirements are basically the same - you need tons of examples of successful problem-solving - but RL gives you a more principled framework for learning optimal behavior.\n\n\n\n\n\nflowchart LR\n    subgraph RLSystem[\"RL System\"]\n        agent[\"Agent\"]\n        exp[\"Experience Buffer\"]\n    end\n\n    subgraph DualSystem[\"Dualformer\"]\n        fast[\"Fast Path\"]\n        slow[\"Slow Path\"]\n        trace[\"Trace Processing\"]\n    end\n\n    subgraph Environment[\"Environment\"]\n        state[\"State Space\"]\n        reward[\"Reward Signal\"]\n    end\n\n    agent --&gt; state\n    state --&gt; reward\n    reward --&gt; agent\n    agent --&gt; exp\n    exp --&gt; trace\n    trace --&gt; fast\n    trace --&gt; slow\n    fast --&gt; agent\n    slow --&gt; agent\n\n    classDef rl fill:#bbf,stroke:#333,stroke-width:2px\n    classDef dual fill:#fbb,stroke:#333,stroke-width:2px\n    classDef env fill:#bfb,stroke:#333,stroke-width:2px\n    \n    class RLSystem rl\n    class DualSystem dual\n    class Environment env\n\n\n\n\n\n\nThere might be something interesting in combining an RL agent with their architecture though. Imagine co-training the agent and the Dualformer so it learns both quick-and-dirty and careful-and-thorough approaches while the agent bumbles its way to solutions. The diagram above shows how this might work - the RL agent feeds experiences into the Dualformer, which learns both fast and slow solving strategies that can then guide future exploration.\nThe paper isn’t bad, and I’m sure it’s interesting and valuable to people smarter than me/folks who actually work on transformers day-to-day - the technical work, at a minimum, is solid. But they successfully annoyed me into reading something that over-promised and under-delivered by wrapping a fairly straightforward transformer training technique in cognitive science gift paper.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nSolid technical contribution wrapped in questionable cognitive science\nProbably should’ve just used RL\nPotential for interesting hybrid architectures (though that wasn’t their point)"
  },
  {
    "objectID": "blog/posts/regulatory_compliance.html",
    "href": "blog/posts/regulatory_compliance.html",
    "title": "Papers: Cost of Regulatory Compliance in the United States",
    "section": "",
    "text": "I’ve always been pretty interested in the distortionary impact of regulation – I think people tend to read the lede on the efficiency losses associated with regulation from a theoretical perspective, which by and large glosses over the consumer benefit tied to information asymmetries, and assume that the distortionary impact inherently makes regulation harmful.Bombardini, Trebbi, and Zhang’s Measuring the Costs and Benefits of Regulation, and Trebbi and Zhang’s The Cost of Regulatory Compliance in the United States is now freely available on the NBER site, and it’s about what you’d expect. It is interesting to work toward stylized facts on the aggregate regulatory burden as it stands today and how it has evolved over time – the authors target only the former – but ultimately it doesn’t tell us much about the benefits. A glib framing might contrast how we discuss regulation with how we discuss infrastructure investment: road work wastes a ton of time every year, but ultimately results in fewer accidents and damage to vehicles so on balance it’s probably a good thing! Broadly, the former outlines where we’re at in terms of evaluating the impact of regulation while the latter implements an interesting method that blends O*Net data with employment data outlining on-the-job activities, identifying tasks that are regulation-related (like “evaluate applications for compliance” or “monitor adherence to safety standards”), figuring out what percentage of each occupation’s time is spent on regulatory tasks, and then calculating what percentage of a company’s total wage bill goes to regulatory compliance. They argue that the cost of regulation exhibits a U-shaped relationship with regulatory costs - medium-sized companies spend the highest percentage on compliance.\n\n[They] observe that over time, the inverted-U shape relation between RegIndex and firm size became stronger. In particular, much of the changes in RegIndex (the authors’ derived measure of the percentage of an establishment’s total labor spending ascribed to performing regulation-related tasks) come from firms with a medium and high level of employment, while there is little change in RegIndex for small firms. Importantly, this enhanced inversed-U relationship between RegIndex and size maps to a greater average log change in RegIndex for larger firms\n\nThis suggests that neither really small companies (who might fly under the radar) nor really big ones (who can achieve economies of scale in compliance) bear the heaviest regulatory burden. From an identification perspective, the authors employ two main identification strategies:\n\nA series of event studies/diff-in-diff around major regulatory changes, like:\n\nCredit CARD Act of 2009 affecting credit card issuers\nEnergy Policy Act of 2005 and subsequent re-regulation of oil & gas\nDear Colleague Letter of 2011 affecting colleges\nACA affecting hospitals\n\nA shift-share instrument approach to separate enforcement from regulatory requirements. They:\n\nMeasure regulatory requirement changes via new regulations from each agency\nMeasure enforcement changes via changes in regulatory-related employment at agencies\nCreate Bartik-style instruments based on industry exposure to different agencies\n\n\nThe event studies seem decent for validation - they show RegIndex moves in expected directions when regulations change. Their measure also captures both increases AND decreases in regulation, but I’m not super convinced by their causal claims about effects on firm size distribution. The inverted-U shape in regulatory costs by firm size is descriptive, and while they try to use their instruments to understand whether it’s driven by enforcement vs requirements, there are still lots of potential confounders. For instance, mid-sized firms might have different business models or operate in different sectors that require more compliance regardless of firm size per se.\nI didn’t have high hopes for this paper when the first hit on Google was a writeup by the authors hosted by Cato, but it was at the very least interesting.\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nKey contribution is new measurement approach: using actual labor allocation data to measure regulatory compliance costs\nFind mid-sized firms (~500 employees) bear highest relative regulatory burden\nIdentification comes from regulatory event studies and shift-share design\nMain limitations are potential overestimation of compliance time and missing non-labor costs\nValidates well against known regulatory changes but causal claims about firm size effects remain uncertain"
  }
]